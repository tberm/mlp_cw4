## Activations are All You Need: LLM Probing as a Method for Hallucination Detection

_[Tomas Bermejo](https://github.com/tberm), [Dylan Ponsford](https://github.com/dylanxp), [Pascual Merita Torres](https://github.com/PascualMeritaTorres)_

This repo contains the code for our project investigating the use of LLM probes as a
practical tool for detecting hallucinations. Our work aims to build on the research by:

- Azaria & Mitchell, 2023 (A&M): http://arxiv.org/abs/2304.13734
- Levinstein & Herrman, 2023 (L&H): https://arxiv.org/abs/2307.00175
- Marks & Tegmark, 2023 (M&T): https://arxiv.org/abs/2310.06824
- Zou et al, 2023: https://arxiv.org/abs/2310.01405

Large parts of the code are copied from the corresponding repos:

- L&H: https://github.com/balevinstein/Probes
- M&T: https://github.com/saprmarks/geometry-of-truth

The above works have demonstrated that one can use a linear probe or simple neural
classifier to predict whether a sentence processed by a LLM is a falsehood, by feeding
the activations of the LLM as input to the probe. Through our experiments, we test:

- how well a classifier trained on the simple true-false dataset of A&M generalises to true and false answers to questions in the [TruthfulQA](https://github.com/sylinrl/TruthfulQA) dataset;
- how well this method performs on potentially incorrect answers generated by the model itself, rather than artificially generated statements;
- whether this method is still effective in detecting falsehoods when the model is primed to be truthful via its prompt. 

### Workflow

#### Preparing the QA data

Some of our experiments involve using LLM-generated TruthfulQA answers as the dataset, so these answers must be generated first. This can be done using the [TruthfulQA](https://github.com/sylinrl/TruthfulQA) evaluation script, which is also included in this repo for convenience.

To generate ground truth true/false labels for the LLM answers, we used OpenAI's GPT-4 to evaluate the answers generated by our LLM. The script ``evaluate_answers.py`` sends question-answer pairs to the OpenAI API together with the example correct and incorrect answers from TruthfulQA, and saves a verdict for each answer.

We found that GPT-4's verdicts were only correct about 80% of the time, so we used ``fix_gpt_evals.py`` to interactively review the full dataset and change any verdicts that appeared wrong.

#### Extracting activations

The script ``get_true_false_embeddings.py`` is copied from the L&H repo and allows extracting LLM activations for true/false statements as in the original work by A&M. The activation values are saved to an output CSV file.

To get activations for TruthfulQA questions and answers, use ``get_qa_probs_and_embeddings.py``. This script also extracts model probabilities and entropy values over the answer tokens, which we use as features for a baseline truthfulness classifier in our work.

#### Training and evaluating classifiers

The high-level ``run_experiment.py`` module allows training and evaluation of a classifier on LLM activations, using one of three model types:

- A three-layer fully-connected neural network ("SAPLMA", from A&M)
- A logistic regression model
- A mean-mass probe, from M&T

One can also run a baseline model trained on features derived from the LLM's outputs (e.g. average probability of tokens in text) using the ``run_prob_baseline`` function. See the module docstring for more details and examples.